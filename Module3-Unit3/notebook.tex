
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Supervised-ML}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{assignment-2.2}{%
\subsection{Assignment 2.2}\label{assignment-2.2}}

\hypertarget{scikit-learn}{%
\subsection{Scikit Learn}\label{scikit-learn}}

Scikit Learn is an open source, simple and efficient tools for data
mining and data analysis. It is built on NumPy, SciPy, and matplotlib
and is accessible to everybody, and reusable in various contexts.

\href{http://scikit-learn.org/stable/}{Scikit-Learn Official}

    \hypertarget{linear-regression}{%
\subsubsection{Linear Regression}\label{linear-regression}}

Linear regression is about finding the approximation of a linear model
used to describe the relationship between two or more variables. In
simple linear regression, there are two variables: a dependent variable
and an independent variable (target variable). The main point in the
linear regression is that our dependent value should be continuous and
cannot be a discrete value. However, the independent variable(s) can be
measured on either a categorical or continuous measurement scale. When
more than one independent variable is present, then it is called
multiple linear regression.

We have already carried out Linear Regression using Matrix
multiplication in Project of Module 1 where we used the Normal Equation
for Linear Regression \(Theta = (X^TX)^{-1}X^T\). Complexity of this
computation will increase as the number of features increases. It gets
very slow when number of features grow large. Thus we'll use other
numeric techniques such as least square methods or Optimisation
technique to miniize the error.

The core idea is to obtain a line or a plane that best fits the data.
The best fit line or plane is the one for which total prediction error
(all data points) are as small as possible. Coefficients of the equation
of this line are what we want to deduce.

\$ y = \Theta\_0 + \Theta\_1 * x\_1 +\Theta\_2 * x\_2 +\ldots{} \$

Notice all the parameters are linear to the weights, hence linear
regression.

\includegraphics{https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png}
Error is the distance between the point to the regression line or plane.
We minimize this error and estimate the optimized value for theta using
least square methods or Optimisation technique such as gradient descent.

\emph{Generally, there are two applications for linear regression.} 1.
Identify the strength of the effect that the independent variables have
on a dependent variable. For example, does nearness to a road or having
a swiming pool or school nearby have any effect on cost of the house?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  To predict the impact of changes. That is, to understand how the
  dependent variable changes when we change the independent variables.
  For example, a multiple linear regression can tell you how much that
  housing prices increase or decrease for every additional square foot
  of the house area, holding other factors constant.
\item
  In multiple linear regression, you can examine which variables are
  significant predictors of the outcome variable.
\end{enumerate}

\href{https://en.wikipedia.org/wiki/Linear_regression}{Linear Regression
WIKI}

    Data has been imported for you. To know more about datasets available in
sklearn, visit this
\href{http://scikit-learn.org/stable/datasets/index.html}{link}.

    \hypertarget{exercise-1}{%
\subsubsection{Exercise 1}\label{exercise-1}}

Import necessary libraries for -
\href{http://scikit-learn.org/stable/modules/linear_model.html}{Linear
Machine Learning Model} -
\href{http://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics}{Accuracy
Metrics} - Use Mean Squared Error - Other Necessary Packages such as
pandas, numpy and pyplot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{linear\PYZus{}model}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{metrics}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    We will use Fuel Consumption rating dataset for this problem. This
Dataset provide model-specific fuel consumption ratings and estimated
carbon dioxide emissions for new light-duty vehicles for retail sale in
Canada.
\href{https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64}{Data
Source}

\hypertarget{exercise-2}{%
\subsubsection{Exercise 2}\label{exercise-2}}

Import the file \textbf{\emph{CO2\_1995.csv}} as a dataframe

A List of Header names has been provided to you as ``head''

Analyze the dataset and perform any data cleaning that seems to be
necessary.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{n}{head} \PY{o}{=} \PY{p}{[} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MODELYEAR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MAKE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MODEL}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{VEHICLECLASS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ENGINESIZE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CYLINDERS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRANSMISSION}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FUELTYPE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CITY\PYZus{}FUELCONSUMPTION}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HWY\PYZus{}FUELCONSUMPTION}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{COMB\PYZus{}FUELCONSUMPTION1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{COMB\PYZus{}FUELCONSUMPTION2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CO2EMISSIONS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./CO2\PYZus{}1995.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{o}{=} \PY{n}{head}
         \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:}   MODELYEAR   MAKE         MODEL VEHICLECLASS ENGINESIZE  CYLINDERS  \textbackslash{}
         1      1995  ACURA       INTEGRA   SUBCOMPACT        1.8        4.0   
         2      1995  ACURA       INTEGRA   SUBCOMPACT        1.8        4.0   
         3      1995  ACURA  INTEGRA GS-R   SUBCOMPACT        1.8        4.0   
         4      1995  ACURA        LEGEND      COMPACT        3.2        6.0   
         5      1995  ACURA  LEGEND COUPE      COMPACT        3.2        6.0   
         
           TRANSMISSION FUELTYPE CITY\_FUELCONSUMPTION HWY\_FUELCONSUMPTION  \textbackslash{}
         1           A4        X                 10.2                 7.0   
         2           M5        X                  9.6                 7.0   
         3           M5        Z                  9.4                 7.0   
         4           A4        Z                 12.6                 8.9   
         5           A4        Z                 13.0                 9.3   
         
           COMB\_FUELCONSUMPTION1 COMB\_FUELCONSUMPTION2 CO2EMISSIONS  
         1                   8.8                    32          202  
         2                   8.4                    34          193  
         3                   8.3                    34          191  
         4                  10.9                    26          251  
         5                  11.3                    25          260  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{analyze-the-dataset-and-implement-linear-regression}{%
\subsection{Analyze the dataset and Implement Linear
Regression}\label{analyze-the-dataset-and-implement-linear-regression}}

Before attempting to fit a linear model to observed data, a modeler
should first determine whether or not there is a relationship between
the variables of interest. This does not necessarily imply that one
variable causes the other, but that there is some significant
correlation between the two variables.

A scatterplot can be a helpful tool in determining the strength of the
relationship between two variables. If there appears to be no
association between the proposed explanatory and dependent variables
(i.e., the scatterplot does not indicate any increasing or decreasing
trends), then fitting a linear regression model to the data probably
will not provide a useful model.

A valuable numerical measure of association between two variables is the
correlation coefficient, which is a value between -1 and 1 indicating
the strength of the association of the observed data for the two
variables. We can use correlation functions for this purpose.

    \hypertarget{exercise-3}{%
\subsubsection{Exercise 3}\label{exercise-3}}

Plot Histogram to check the distribution of the data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{n}{variables} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CYLINDERS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ENGINESIZE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CITY\PYZus{}FUELCONSUMPTION}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{target} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CO2EMISSIONS}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{X} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{variables}\PY{p}{]}
         \PY{n}{X}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{exercise-4}{%
\subsubsection{Exercise 4}\label{exercise-4}}

Assign the target values to y.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{y} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{target}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    Let's now plot a scatter plot to check if the chosen variables can be
used for linear relationship.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{n}{nv} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{variables}\PY{p}{)}
         \PY{n}{s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{nv}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{nv}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{s}\PY{p}{,}\PY{n}{s}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{wspace} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{,}\PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{variables}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{n}{target}\PY{p}{]}\PY{p}{,}  \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{variables}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{target}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{split-the-data-into-train-validation-and-test-set}{%
\subsubsection{Split the data into train, validation and test
set}\label{split-the-data-into-train-validation-and-test-set}}

Let us now
\href{http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html}{split
the data into a training and testing set} which will allow you to
produce two mutually exclusive sets. We will use train\_test\_split
twice to split data into training, validation and testing set. Training
set would be used for training the model, validation set would be used
for tuning the hyperparameters while testing set will be used to
evaluate the accuracy and see if your model generalizes well. This means
whether the model is able to perform well on data it has not seen
before. This is a way to see if the model is overfitting.

Overfitting is when a model is trained too well on the training data.
You want to avoid overfitting, as this would mean that the model mostly
just memorized the training data. This would account for a large
accuracy with the training data but a low accuracy in the testing data.

    Note: We will be using the same random\_seed in all our model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{} Run this cell to set a uniform random\PYZus{}seed}
         \PY{n}{random\PYZus{}seed} \PY{o}{=} \PY{l+m+mi}{7}
\end{Verbatim}


    \hypertarget{exercise-5}{%
\subsubsection{Exercise 5}\label{exercise-5}}

Split our data to train and test with the test size of 0.25 and
random\_state=random\_seed.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{n}{Train\PYZus{}set} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{Test\PYZus{}set} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}
         \PY{n}{Train\PYZus{}set} \PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{Test\PYZus{}set} \PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{exercise-6}{%
\subsubsection{Exercise 6}\label{exercise-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create Linear Regression object with fit\_intercept set to True.
\item
  Train the model using the training sets.
\item
  Save coefficients in Theta.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{n}{Theta} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{Regressor} \PY{o}{=} \PY{n}{linear\PYZus{}model}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{Theta} \PY{o}{=} \PY{n}{Regressor}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n}{intercept} \PY{o}{=} \PY{n}{Regressor}\PY{o}{.}\PY{n}{intercept\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{intercept}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
-3.647805995880232
7.394097673385936
18.04527737591793
19.91206719946885

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    Let's tune the hyperparamters in the validation set.

    \hypertarget{exercise-7}{%
\subsubsection{Exercise 7}\label{exercise-7}}

Find the parameters currently used by our Linear Regression model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{n}{lr\PYZus{}params} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{lr\PYZus{}params} \PY{o}{=} \PY{n}{Regressor}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{lr\PYZus{}params}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\{'copy\_X': True, 'fit\_intercept': True, 'n\_jobs': 1, 'normalize': False\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    As we have only 4 parameters and all the parameters seems ok so, let's
test our linear regression model on test set.

    \hypertarget{exercise-8}{%
\subsubsection{Exercise 8}\label{exercise-8}}

Predict the values of the test set in predict\_Y.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{n}{predict\PYZus{}Y} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{predict\PYZus{}Y} \PY{o}{=} \PY{n}{Regressor}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{residual-analysis}{%
\subsubsection{Residual Analysis}\label{residual-analysis}}

Randomness and unpredictability are the two main components of a
regression model.

Prediction = Deterministic + Stochastic

Deterministic part is easily covered by the predictor variable in the
model. Stochastic part reveals the fact that the expected and observed
value is unpredictable. There will always be some information that are
missed to cover. This difference in information can be obtained as the
residual information. Residual = predicted - target

Residual plot helps in analyzing the model using the values of residues.
It is plotted between predicted values and residue. Their values are
standardized. The distance of the point from 0 specifies how bad the
prediction was for that value. If the value is positive, then the
prediction is low. If the value is negative, then the prediction is
high. 0 value indicates prefect prediction. Detecting residual pattern
can improve the model.

Non-random pattern of the residual plot indicates that the model is,

\begin{itemize}
\tightlist
\item
  Missing a variable which has significant contribution to the model
  target
\item
  Missing to capture non-linearity (using polynomial term)
\item
  No interaction between terms in model
\end{itemize}

Characteristics of a residue

\begin{itemize}
\tightlist
\item
  Residuals do not exhibit any pattern
\item
  Adjacent residuals should not be same as they indicate that there is
  some information missed by system.
\end{itemize}

    Now, let's plot a scatterplot of residual with respect to the predicted
output.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{n}{figRes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{predict\PYZus{}Y}\PY{p}{,} \PY{n}{predict\PYZus{}Y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{)}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{hlines}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{xmin}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{xmax}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residual Plot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residual}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{pass}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{metric-for-model-evaluation}{%
\subsubsection{Metric for Model
Evaluation}\label{metric-for-model-evaluation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sum of Square Errors(SSE): This gives information about how far
  estimated regression line is from the best fit or regression
  line/plane .
\end{enumerate}

\$SSE = \Sigma\^{}n\_\{i=1\} (y\_\{actual\} -y\_\{predicted\})\^{}2 \$

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Total Sum of Squares (SSTO): This gives information about how much the
  data point move around the mean.
\end{enumerate}

\$SSTO = \Sigma\^{}n\_\{i=1\} (y\_\{actual\}
-y\_\{actual\_average\})\^{}2 \$

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  R-Squared Value
  \href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{\(R^2\)
  or Coefficient of Determination}
\end{enumerate}

\(R^2 = 1 - \frac{SSE}{SSTO}\)

R-squared value ranges from 0 to 1. Value `1' indicates predictor
perfectly accounts for all the variation in Y. Value `0' indicates that
predictor `x' accounts for no variation in `y'. Thus, it provides a
measure of how well observed outcomes are replicated by the model.

    \hypertarget{exercise-9}{%
\subsubsection{Exercise 9}\label{exercise-9}}

Use sklearn r2\_score to evaluate the model prediction.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{R2Score} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{R2Score} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predict\PYZus{}Y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{lasso-ridge-regression}{%
\subsubsection{LASSO \& RIDGE Regression}\label{lasso-ridge-regression}}

Lasso and ridge regression are powerful extension for linear regression
generally used in presence of large number of features. These techniques
reduces the risk of overfitting and reduce computational challenges.Both
of them work by penalizing the magnitude of coefficients of features
along with minimizing the error between predicted and actual
observations. So, these are called `regularization' techniques. The key
difference is in how they assign penalty to the coefficients:

Lasso Regression: Performs L1 regularization, i.e.~adds penalty
equivalent to absolute value of the magnitude of coefficients
Minimization objective = Least squares objective + α * (sum of absolute
value of coefficients)

Ridge Regression: Performs L2 regularization, i.e.~adds penalty
equivalent to square of the magnitude of coefficients Minimization
objective = Least squares objective + α * (sum of square of
coefficients)

Now, let's create lasso regression object.

\hypertarget{exercise-10}{%
\subsubsection{Exercise 10}\label{exercise-10}}

1. Create lasso regression object, regr\_Lasso with fit\_intercept set
to True, alpha=50 and random\_state as the random\_seed. 2. Train the
model using the training sets. 3. Save coefficients in Theta\_Lasso.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{regr\PYZus{}Lasso} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{Theta\PYZus{}Lasso} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{regr\PYZus{}Lasso} \PY{o}{=} \PY{n}{linear\PYZus{}model}\PY{o}{.}\PY{n}{Lasso}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}
         \PY{n}{regr\PYZus{}Lasso}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{Theta\PYZus{}Lasso} \PY{o}{=} \PY{n}{regr\PYZus{}Lasso}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Theta\PYZus{}Lasso}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
14.46124394115535

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{exercise-11}{%
\subsubsection{Exercise 11}\label{exercise-11}}

Calculate r2\_score for LASSO Regression.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{n}{r2score\PYZus{}LASSO} \PY{o}{=} \PY{k+kc}{None}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{r2score\PYZus{}LASSO} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{regr\PYZus{}Lasso}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{r2score\PYZus{}LASSO}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.9075072073291988

    \end{Verbatim}

    You must have got the r2score of approximately 0.91.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}


    \hypertarget{grid-search-for-alpha-using-a-logarithmically-spaced-array}{%
\subsubsection{Grid Search for Alpha using a logarithmically spaced
array}\label{grid-search-for-alpha-using-a-logarithmically-spaced-array}}

Let us now do the grid search over alphas and find the best parameter.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{n}{alphas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alpha}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{alphas}\PY{p}{\PYZcb{}}
         \PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{linear\PYZus{}model}\PY{o}{.}\PY{n}{Lasso}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{)}
         \PY{n}{myfit} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{myfit}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{myfit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.9628664869538451
Lasso(alpha=1.023292992280754, copy\_X=True, fit\_intercept=True, max\_iter=1000,
   normalize=False, positive=False, precompute=False, random\_state=None,
   selection='cyclic', tol=0.0001, warm\_start=False)

    \end{Verbatim}

    So, the best value of alpha is approx. 1.023. We can also see our
best\_score increasing after using grid search.

    \hypertarget{exercise-12}{%
\subsubsection{Exercise 12}\label{exercise-12}}

Calculate the r2\_score of our new predicted values using grid search in
r2\_score\_lasso.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{n}{r2\PYZus{}score\PYZus{}lasso} \PY{o}{=} \PY{k+kc}{None}
          \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
          \PY{n}{r2\PYZus{}score\PYZus{}lasso} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{myfit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
          \PY{n}{r2\PYZus{}score\PYZus{}lasso}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}100}]:} 0.9690211342871922
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} INTENTIONALLY LEFT BLANK \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} BEGIN HIDDEN TEST}
         \PY{k}{assert}\PY{p}{(}\PY{n}{r2\PYZus{}score\PYZus{}lasso} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.9}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END HIDDEN TEST}
\end{Verbatim}


    Congratulations, you have now implemented simple linear regression,
lasso\_regression and lasso\_regression with the best hyperparameters
using grid search.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
